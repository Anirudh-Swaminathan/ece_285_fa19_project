{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python2 and python3 compatibility between loaded modules\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports go here\n",
    "%matplotlib inline\n",
    "\n",
    "# Reading files\n",
    "import os\n",
    "\n",
    "# Vector manipulations\n",
    "import numpy as np\n",
    "\n",
    "# DL framework\n",
    "# torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data as td\n",
    "import torchvision as tv\n",
    "\n",
    "# Plotting images\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# COCO loading captions\n",
    "from pycocotools.coco import COCO\n",
    "import skimage.io as io\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "\n",
    "# import created vocabulary\n",
    "from vocab_creator import VocabCreate as vc\n",
    "\n",
    "# PIL Image\n",
    "from PIL import Image\n",
    "\n",
    "# regex for captions\n",
    "import re\n",
    "\n",
    "\"\"\"\n",
    "# evaluation metrics on MSCOCO dataset\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.spice.spice import Spice\n",
    "\"\"\"\n",
    "\n",
    "# json for dumping stuff onto files as output\n",
    "import json\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "dataset_root_dir = '/datasets/COCO-2015/'\n",
    "annotations_root_dir = '../datasets/COCO/annotations/'\n",
    "train_dir = \"train2014\"\n",
    "val_dir = \"val2014\"\n",
    "test_dir = \"test2015\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data annotations\n",
    "train_ann = \"{}captions_{}.json\".format(annotations_root_dir, train_dir)\n",
    "coco_train_caps = COCO(train_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data annotations\n",
    "val_ann = \"{}captions_{}.json\".format(annotations_root_dir, val_dir)\n",
    "coco_val_caps = COCO(val_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the image IDs\n",
    "train_image_ids = coco_train_caps.getImgIds()\n",
    "# loadImgs() returns all the images\n",
    "train_imgs = coco_train_caps.loadImgs(train_image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_imgs), len(train_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the val image ids\n",
    "val_image_ids = coco_val_caps.getImgIds()\n",
    "# loadImgs() returns all the images\n",
    "val_imgs = coco_val_caps.loadImgs(val_image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read 1 random training image from file \n",
    "rand_id = np.random.randint(0, len(train_imgs))\n",
    "rand_img = io.imread('{}/{}/{}'.format(dataset_root_dir, train_dir, train_imgs[rand_id]['file_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(rand_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load caption for this particular image\n",
    "ann_id = coco_train_caps.getAnnIds(imgIds=train_imgs[rand_id]['id'])\n",
    "anns = coco_train_caps.loadAnns(ann_id)\n",
    "coco_train_caps.showAnns(anns)\n",
    "f = plt.figure()\n",
    "plt.imshow(rand_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anns)\n",
    "print(len(anns))\n",
    "print(anns[0])\n",
    "print(anns[0]['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read 1 random testing image from file\n",
    "val_rand_id = np.random.randint(0, len(val_imgs))\n",
    "val_rand_img = io.imread('{}/{}/{}'.format(dataset_root_dir, val_dir, val_imgs[val_rand_id]['file_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(val_rand_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load caption for this particular image\n",
    "ann_id = coco_val_caps.getAnnIds(imgIds=val_imgs[val_rand_id]['id'])\n",
    "anns = coco_val_caps.loadAnns(ann_id)\n",
    "coco_val_caps.showAnns(anns)\n",
    "f = plt.figure()\n",
    "plt.imshow(val_rand_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class\n",
    "class COCODataset(td.Dataset):\n",
    "    \n",
    "    \n",
    "    def __init__(self, dataset_root_dir, annotations_root_dir, vocab, mode=\"train2014\", image_size=(224, 224)):\n",
    "        super(COCODataset, self).__init__()\n",
    "        self.dataset_root_dir = dataset_root_dir\n",
    "        self.annotations_root_dir = annotations_root_dir\n",
    "        self.image_size = image_size\n",
    "        self.mode = mode\n",
    "        # training data annotations\n",
    "        self.ann = \"{}captions_{}.json\".format(annotations_root_dir, mode)\n",
    "        self.coco_caps = COCO(self.ann)\n",
    "        # get all the image IDs\n",
    "        self.image_ids = self.coco_caps.getImgIds()\n",
    "        self.ann_ids = list(self.coco_caps.anns.keys())\n",
    "        # loadImgs() returns all the images\n",
    "        self.imgs = self.coco_caps.loadImgs(self.image_ids)\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ann_ids)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"COCODataset(mode={}, image_size={})\". \\\n",
    "        format(self.mode, self.image_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ann_id = self.ann_ids[idx]\n",
    "        cap = self.coco_caps.anns[ann_id][\"caption\"]\n",
    "        img_id = self.coco_caps.anns[ann_id][\"image_id\"]\n",
    "        img_path = self.coco_caps.loadImgs(img_id)[0][\"file_name\"]\n",
    "        \n",
    "        img = Image.open('{}/{}/{}'.format(self.dataset_root_dir, self.mode, img_path))\n",
    "        img = img.convert('RGB')\n",
    "        transform = tv.transforms.Compose([\n",
    "            tv.transforms.Resize(self.image_size),\n",
    "            tv.transforms.RandomHorizontalFlip(),\n",
    "            tv.transforms.ToTensor(),\n",
    "            tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        x = transform(img)\n",
    "        \n",
    "        # return caption\n",
    "        cap = str(cap)\n",
    "        clean_cap = re.sub(r'[^a-zA-Z0-9 ]+', '', cap)\n",
    "        word_list = clean_cap.lower().strip().split()\n",
    "        for i in range(len(word_list)):\n",
    "            if word_list[i] not in vocab.one_hot_inds:\n",
    "                word_list[i]=\"unk_vec\"\n",
    "        d = torch.Tensor([vocab.one_hot_inds[\"start_vec\"]]\n",
    "                               + [vocab.one_hot_inds[w] for w in word_list]\n",
    "                               + [vocab.one_hot_inds[\"end_vec\"]]\n",
    "        )\n",
    "        return x, d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the vocabulary\n",
    "# or Create and save to output\n",
    "dict_path = \"../outputs/vocab.npz\"\n",
    "vocab = vc(train_ann, dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the cocodataset\n",
    "training_dataset = COCODataset(dataset_root_dir, annotations_root_dir, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myimshow(image, ax=plt):\n",
    "    image = image.to('cpu').numpy()\n",
    "    image = np.moveaxis(image, [0, 1, 2], [2, 0, 1])\n",
    "    image = (image + 1) / 2\n",
    "    image[image<0] = 0\n",
    "    image[image>1] = 1\n",
    "    h = ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, cap = training_dataset.__getitem__(47)\n",
    "print(cap)\n",
    "myimshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cap.numpy().astype(int)\n",
    "print(cap, cap.dtype)\n",
    "print(type(list(cap)))\n",
    "captions = [vocab.dict[cap[c]] for c in range(len(cap))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(len(cap)):\n",
    "    print(cap[c], type(cap[c]))\n",
    "    print(vocab.dict[cap[c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = \"hellconvention\"\n",
    "clean_cap = re.sub(r'[^a-zA-Z0-9 ]+', '', cap)\n",
    "word_list = clean_cap.lower().strip().split()\n",
    "for i in range(len(word_list)):\n",
    "    if word_list[i] not in vocab.one_hot_inds:\n",
    "        word_list[i]=\"unk_vec\"\n",
    "\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
