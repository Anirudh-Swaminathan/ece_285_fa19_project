{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python2 and python3 compatibility between loaded modules\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports go here\n",
    "%matplotlib inline\n",
    "\n",
    "# Reading files\n",
    "import os\n",
    "\n",
    "# Vector manipulations\n",
    "import numpy as np\n",
    "\n",
    "# DL framework\n",
    "# torch\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.data as td\n",
    "import torchvision as tv\n",
    "\n",
    "# Plotting images\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# COCO loading captions\n",
    "from pycocotools.coco import COCO\n",
    "import skimage.io as io\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (8.0, 10.0)\n",
    "\n",
    "# import created vocabulary\n",
    "from vocab_creator import VocabCreate as vc\n",
    "\n",
    "# PIL Image\n",
    "from PIL import Image\n",
    "\n",
    "# regex for captions\n",
    "import re\n",
    "\n",
    "# import nntools\n",
    "import nntools as nt\n",
    "\n",
    "\"\"\"\n",
    "# evaluation metrics on MSCOCO dataset\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "from pycocoevalcap.tokenizer.ptbtokenizer import PTBTokenizer\n",
    "from pycocoevalcap.bleu.bleu import Bleu\n",
    "from pycocoevalcap.meteor.meteor import Meteor\n",
    "from pycocoevalcap.rouge.rouge import Rouge\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "from pycocoevalcap.spice.spice import Spice\n",
    "\"\"\"\n",
    "\n",
    "# json for dumping stuff onto files as output\n",
    "import json\n",
    "from json import encoder\n",
    "encoder.FLOAT_REPR = lambda o: format(o, '.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading\n",
    "dataset_root_dir = '/datasets/COCO-2015/'\n",
    "annotations_root_dir = '../datasets/COCO/annotations/'\n",
    "train_dir = \"train2014\"\n",
    "val_dir = \"val2014\"\n",
    "test_dir = \"test2015\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data annotations\n",
    "train_ann = \"{}captions_{}.json\".format(annotations_root_dir, train_dir)\n",
    "coco_train_caps = COCO(train_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data annotations\n",
    "val_ann = \"{}captions_{}.json\".format(annotations_root_dir, val_dir)\n",
    "coco_val_caps = COCO(val_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the image IDs\n",
    "train_image_ids = coco_train_caps.getImgIds()\n",
    "# loadImgs() returns all the images\n",
    "train_imgs = coco_train_caps.loadImgs(train_image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_imgs), len(train_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the val image ids\n",
    "val_image_ids = coco_val_caps.getImgIds()\n",
    "# loadImgs() returns all the images\n",
    "val_imgs = coco_val_caps.loadImgs(val_image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read 1 random training image from file \n",
    "rand_id = np.random.randint(0, len(train_imgs))\n",
    "rand_img = io.imread('{}/{}/{}'.format(dataset_root_dir, train_dir, train_imgs[rand_id]['file_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(rand_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load caption for this particular image\n",
    "ann_id = coco_train_caps.getAnnIds(imgIds=train_imgs[rand_id]['id'])\n",
    "anns = coco_train_caps.loadAnns(ann_id)\n",
    "coco_train_caps.showAnns(anns)\n",
    "f = plt.figure()\n",
    "plt.imshow(rand_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(anns)\n",
    "print(len(anns))\n",
    "print(anns[0])\n",
    "print(anns[0]['caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read 1 random testing image from file\n",
    "val_rand_id = np.random.randint(0, len(val_imgs))\n",
    "val_rand_img = io.imread('{}/{}/{}'.format(dataset_root_dir, val_dir, val_imgs[val_rand_id]['file_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axis('off')\n",
    "plt.imshow(val_rand_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load caption for this particular image\n",
    "ann_id = coco_val_caps.getAnnIds(imgIds=val_imgs[val_rand_id]['id'])\n",
    "anns = coco_val_caps.loadAnns(ann_id)\n",
    "coco_val_caps.showAnns(anns)\n",
    "f = plt.figure()\n",
    "plt.imshow(val_rand_img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset class\n",
    "class COCODataset(td.Dataset):\n",
    "    \n",
    "    \n",
    "    def __init__(self, dataset_root_dir, annotations_root_dir, vocab, mode=\"train2014\", image_size=(224, 224)):\n",
    "        super(COCODataset, self).__init__()\n",
    "        self.dataset_root_dir = dataset_root_dir\n",
    "        self.annotations_root_dir = annotations_root_dir\n",
    "        self.image_size = image_size\n",
    "        self.mode = mode\n",
    "        # training data annotations\n",
    "        self.ann = \"{}captions_{}.json\".format(annotations_root_dir, mode)\n",
    "        self.coco_caps = COCO(self.ann)\n",
    "        # get all the image IDs\n",
    "        self.image_ids = self.coco_caps.getImgIds()\n",
    "        self.ann_ids = list(self.coco_caps.anns.keys())\n",
    "        # loadImgs() returns all the images\n",
    "        self.imgs = self.coco_caps.loadImgs(self.image_ids)\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ann_ids)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"COCODataset(mode={}, image_size={})\". \\\n",
    "        format(self.mode, self.image_size)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ann_id = self.ann_ids[idx]\n",
    "        cap = self.coco_caps.anns[ann_id][\"caption\"]\n",
    "        img_id = self.coco_caps.anns[ann_id][\"image_id\"]\n",
    "        img_path = self.coco_caps.loadImgs(img_id)[0][\"file_name\"]\n",
    "        \n",
    "        img = Image.open('{}/{}/{}'.format(self.dataset_root_dir, self.mode, img_path))\n",
    "        img = img.convert('RGB')\n",
    "        transform = tv.transforms.Compose([\n",
    "            tv.transforms.Resize(self.image_size),\n",
    "            tv.transforms.RandomHorizontalFlip(),\n",
    "            tv.transforms.ToTensor(),\n",
    "            tv.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        x = transform(img)\n",
    "        \n",
    "        # return caption\n",
    "        cap = str(cap)\n",
    "        clean_cap = re.sub(r'[^a-zA-Z0-9 ]+', '', cap)\n",
    "        word_list = clean_cap.lower().strip().split()\n",
    "        for i in range(len(word_list)):\n",
    "            if word_list[i] not in vocab.one_hot_inds:\n",
    "                word_list[i]=\"unk_vec\"\n",
    "        d = torch.Tensor([vocab.one_hot_inds[\"start_vec\"]]\n",
    "                               + [vocab.one_hot_inds[w] for w in word_list]\n",
    "                               + [vocab.one_hot_inds[\"end_vec\"]]\n",
    "        )\n",
    "        return x, d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load the vocabulary\n",
    "# or Create and save to output\n",
    "dict_path = \"../outputs/vocab.npz\"\n",
    "vocab = vc(train_ann, dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of the cocodataset\n",
    "training_dataset = COCODataset(dataset_root_dir, annotations_root_dir, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myimshow(image, ax=plt):\n",
    "    image = image.to('cpu').numpy()\n",
    "    image = np.moveaxis(image, [0, 1, 2], [2, 0, 1])\n",
    "    image = (image + 1) / 2\n",
    "    image[image<0] = 0\n",
    "    image[image>1] = 1\n",
    "    h = ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, cap = training_dataset.__getitem__(47)\n",
    "print(cap)\n",
    "myimshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cap.numpy().astype(int)\n",
    "print(cap, cap.dtype)\n",
    "print(type(list(cap)))\n",
    "captions = [vocab.dict[cap[c]] for c in range(len(cap))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in range(len(cap)):\n",
    "    print(cap[c], type(cap[c]))\n",
    "    print(vocab.dict[cap[c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = \"hellconvention\"\n",
    "clean_cap = re.sub(r'[^a-zA-Z0-9 ]+', '', cap)\n",
    "word_list = clean_cap.lower().strip().split()\n",
    "for i in range(len(word_list)):\n",
    "    if word_list[i] not in vocab.one_hot_inds:\n",
    "        word_list[i]=\"unk_vec\"\n",
    "\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the dataloader to be used\n",
    "# collate_fn - to pad all the vectors to the same length\n",
    "def collate_function(data):\n",
    "    data.sort(key=lambda x:len(x[1]), reverse=True)\n",
    "    img, cap=zip(*data)\n",
    "\n",
    "    #stack images\n",
    "    img = torch.stack(img, 0)\n",
    "\n",
    "    #concatenate all captions\n",
    "    cap_lens = [len(c) for c in cap]\n",
    "\n",
    "    #pad all captions to max caption length\n",
    "    padded_caption = torch.zeros(len(cap),max(cap_lens)).long()\n",
    "    for i, c in enumerate(cap):\n",
    "        c_len = cap_lens[i]\n",
    "        padded_caption[i,:c_len] = c[:c_len]\n",
    "\n",
    "    return img,padded_caption, cap_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = td.DataLoader(training_dataset, batch_size=128, shuffle=True, pin_memory=True, collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_loader), len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index to caption\n",
    "def index_to_cap(labs):\n",
    "    \"\"\"Index to caption\"\"\"\n",
    "    cap = labs.numpy().astype(int)\n",
    "    caps = [vocab.dict[cap[c]] for c in range(len(cap))]\n",
    "    caps = caps[1:-1]\n",
    "    caption = \" \".join(caps)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 1st image and label pair for the 1st 4 minibatches\n",
    "fig, axes = plt.subplots(ncols=2)\n",
    "fig.suptitle(\"1st image for 1st 2 minibatches\")\n",
    "\n",
    "for bind, mbat in enumerate(train_loader):\n",
    "    # print(len(mbat))\n",
    "    # print(type(mbat[0]), type(mbat[1]))\n",
    "    # print(mbat[0].size(), mbat[1].size())\n",
    "    \n",
    "    # The dataloader returns both the image and the caption\n",
    "    # image is in the 0th index, caption is 1st index\n",
    "    img = mbat[0][0, :, :, :]\n",
    "    lab = mbat[1][0]\n",
    "    capt = index_to_cap(lab)\n",
    "    # print(lab.item())\n",
    "    myimshow(img, ax=axes[bind])\n",
    "    #axes[bind].text(50, 250, \"label: {}\".format(caption), size=12, verticalalignment='center')\n",
    "    # axes[bind].set_ylabel(\"label: {}\".format(lab.item()))\n",
    "    axes[bind].set_title(\"mini-batch {}\".format(bind+1))\n",
    "    print(capt)\n",
    "    if bind == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation dataset\n",
    "val_dataset = COCODataset(dataset_root_dir, annotations_root_dir, vocab, mode=val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = td.DataLoader(val_dataset, batch_size=128, shuffle=False, pin_memory=True, collate_fn=collate_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
